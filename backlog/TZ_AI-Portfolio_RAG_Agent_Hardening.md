# AI‑Portfolio / rag-api-new — анализ проблем и обновлённое ТЗ (v2)

Дата: 2025‑12‑21

## 1) Краткий вывод по текущему пайплайну

Текущий агент в целом уже даёт полезные ответы по портфолио, но есть **несколько критичных дефектов**, которые приводят к:
- **выходу за рамки роли** (агент начинает “болтать не по теме”, сочинять/развлекать);
- **ошибочным фактам** даже при наличии правильных данных в индексе;
- **галлюцинациям/предположениям** (“вероятно MySQL”), что подрывает доверие;
- **неустойчивости планирования** (ошибки structured output и неконсистентные render_style).

Ниже — причины (по логам) и конкретные требования к улучшениям.

---

## 2) Серьёзные проблемы (симптом → причина → риск)

### P0. Агент не держит роль (оффтоп/сказки)
**Симптом:** на запрос “расскажи сказку” агент начинает писать сказку вместо вежливого отказа и возврата к теме портфолио.

**Вероятная причина:**
- отсутствует **явная “ScopeGuard” стадия** (детерминированная или LLM‑классификация), которая *до* планирования отсекает запросы вне домена “портфолио Дмитрия”;
- системные инструкции/политики не содержат жёсткого правила “вне роли — откажись”.

**Риск:** пользователь может увести агента в любую тему; затем ошибки/галлюцинации и неконтролируемый контент.

---

### P0. Ответы могут быть фактически неверными из‑за неправильной агрегации фактов (пример: “АЛОР”)
**Симптом:** на вопрос “чем занимался Дмитрий в компании АЛОР” агент отвечает, что Дмитрий “работал биржевым брокером”, хотя в портфолио это описание **проекта/компании**, а не роль Дмитрия.

**Причина (по логике пайплайна):**
- план выбирает `intent=project_details` для `entity=company:alor`;
- `graph_query_tool` возвращает **описание проекта** (АЛОР БРОКЕР), и это попадает в `rendered_facts`;
- self-check действительно находит правильный документ с ролью (“Python Backend Developer… Разрабатывал…”) — но **в финальные “факты для Answer LLM” он не попадает** либо отфильтровывается/теряется на этапе построения `rendered_facts`;
- Answer LLM, следуя правилу “только факты”, честно воспроизводит **не тот факт**.

**Риск:** агент может систематически “переприсваивать” описания проектов пользователю (ошибочная биография).

---

### P0. Галлюцинации/предположения в ответах (пример: MySQL)
**Симптом:** на вопрос “какие БД использовал” агент пишет “можно предположить… SQL Server / PostgreSQL / MySQL”, хотя MySQL отсутствует в портфолио.

**Причины:**
- в “Найденных фактах” для этого вопроса оказались **не БД**, а просто список “Технологии” (без PostgreSQL/SQL Server в топ‑10), поэтому модель “добирает” ответ предположениями;
- в подсказках Answer LLM нет жёсткого запрета на догадки (есть фраза “используй только факты”, но модель всё равно пытается быть “полезной”).

**Риск:** подмешивание выдуманных технологий/фактов.

---

### P1. Нет таксономии технологий → ответы смешивают “языки/фреймворки/инструменты/БД”
**Симптом:** на “какие языки программирования знает” агент перечисляет `.NET Core`, `Next.js`, `Alembic` и т.п. как “языки”.

**Причина:**
- `technology_overview` возвращает общий список технологий без типа (language/db/framework/tool…);
- отсутствуют фильтры в `graph_query_tool` или модель данных для “тип технологии”.

**Риск:** неправильные ответы даже при хорошей Retrieval базе.

---

### P1. Нестабильность structured output у Planner LLM
**Симптом:** в логах есть падения парсинга `QueryPlanV2` из‑за несовпадений enum (например `bulleted_list` вместо допустимого значения), из‑за чего включается “repair” и падает уверенность.

**Причина:**
- Planner prompt не фиксирует строго допустимые значения;
- отсутствует “tolerant mapping” синонимов на уровне кода.

**Риск:** планирование деградирует “случайно” и становится менее управляемым.

---

## 3) Что уже хорошо (сохраняем)
- Тон общения и краткость в большинстве ответов — ок.
- Общий подход “Planner → Tools → Answer” и self-check критик — правильный.
- Для “где работает/где работал” ответы получаются ровными и полезными.

---

## 4) Обновлённое ТЗ (v2): требования к улучшениям

### 4.1 Цель
Сделать агента **строго портфолио‑ориентированным**, **без галлюцинаций**, с **типизированными ответами** (языки ≠ БД ≠ фреймворки), и с устойчивым планированием.

### 4.2 Нефункциональные требования (P0)
1. **Запрет на предположения.**  
   Если в фактах нет ответа — агент отвечает:  
   “В данных портфолио это не найдено. Могу перечислить то, что точно указано, или уточни вопрос.”

2. **Запрет на оффтоп.**  
   Любые запросы не про портфолио Дмитрия → вежливый отказ + предложение вариантов вопросов по портфолио.

3. **Grounding‑проверка ответа (verifier).**  
   После генерации ответа выполняется проверка:  
   - каждый “именованный факт” (технология/компания/проект/должность/дата) должен присутствовать в `facts/items`;  
   - иначе: автоматический rewrite “строго по фактам” **или** отказ “нет данных”.

4. **Детерминированность для “высокорисковых” интентов** (контакты, место работы, список БД/языков):  
   использовать шаблоны/форматирование и минимальную свободу модели.

---

### 4.3 Новый шаг пайплайна: ScopeGuard (P0)
**Перед Planner LLM** добавить шаг `scope_guard(question, thread_context) -> ScopeDecision`:

- `IN_SCOPE`: вопрос про портфолио (опыт, проекты, технологии, достижения, контакты, образование, навыки, стек, архитектура проектов).
- `OUT_OF_SCOPE`: сказки, развлечения, общие знания, “посоветуй фильм”, политика, медицина и т.п.

**Поведение:**
- `OUT_OF_SCOPE` → ответ **без вызова инструментов**, шаблон:
  - 1 фраза отказа (вежливо),
  - 1–3 подсказки “что спросить по портфолио”.

**Требование:** `OUT_OF_SCOPE` нельзя “перебить” последующими шагами.

---

### 4.4 Таксономия технологий (P0)
Ввести типизацию `Technology.category`:

- `language`
- `database`
- `framework`
- `ml_framework` (опционально)
- `tool`
- `cloud`
- `library`
- `other`

**Где хранить:**
- в Postgres (content-api) + отдавать в ingest,
- и/или в графе (nodes: technology) как `category`.

**Требование:** на запросы:
- “какие языки знает” → только `category=language`;
- “какие БД использовал” → только `category=database`;
- “какой стек/технологии” → можно группировать по категориям.

---

### 4.5 Исправление запросов к графу: новые intents (P0)
Добавить/уточнить intents Planner’а и поддержку в `graph_query_tool`:

1. `experience_by_company`  
   Вход: `company_id` → Выход: роль, период, обязанности, достижения (из Experience doc), + связанные проекты.

2. `databases_overview`  
   Вход: person/global → Выход: список технологий `category=database` + “используется в проектах”.

3. `languages_overview`  
   Вход: person/global → Выход: список `category=language`.

4. `rag_usage`  
   Вход: person/global → Выход: проекты/контексты, где указан RAG + кратко “как применял”.

**Запрет:** для вопросов “про компанию/роль” нельзя отвечать только `project_details` (как в кейсе ALOR).

---

### 4.6 Self-check должен улучшать *факты*, а не только “evidence” (P0)
Если critic запускает `portfolio_search_tool`, то найденные документы должны:
- попадать в единый “evidence pool”,
- пройти extraction в `facts` (или минимум — в “ключевые факты”),
- и учитываться в `rendered_facts`, иначе self-check бессмысленен.

**Требование:** после self-check “role/обязанности” обязаны попадать в `rendered_facts` для intents про опыт.

---

### 4.7 Устойчивость Planner structured output (P1)
1. В prompt перечислить **строго допустимые значения** `render_style`, `answer_style`, `fallback.when`.
2. В коде добавить `normalize_plan()`:
   - `bulleted_list` → `bullets`,
   - другие известные синонимы → канонические значения,
   - если поля отсутствуют → заполнить дефолтами.

---

### 4.8 Лимиты и отбор фактов (P1)
Проблема “top‑10 технологий” может выкидывать БД/ключевые элементы.

Требования:
- для списков по категории (`databases_overview`, `languages_overview`) лимитировать **после фильтрации по категории**, а не до.
- для ответов “списком” показывать минимум 1–N, но **не смешивать типы**.

---

## 5) Приёмочные тесты (минимальный регрессионный набор)

### A. Scope
- Q: “расскажи сказку” → отказ + предложение спросить про проекты/опыт.
- Q: “как дела?” → коротко + подсказка, что агент про портфолио.

### B. Базы данных
- Q: “какие БД использовал?” → только БД из портфолио (без MySQL/догадок).

### C. Компания ALOR
- Q: “чем занимался в компании АЛОР?” → “Python Backend Developer … обязанности/достижения …” (НЕ “работал брокером”).

### D. Языки
- Q: “какие языки программирования знает?” → только языки (Python, C# …), без Next.js/.NET/Alembic.

### E. RAG
- Q: “где применял RAG?” → перечисление проектов + 1–2 детали по каждому.

---

## 6) План работ (в порядке приоритета)

**Этап 1 (P0):** ScopeGuard + запрет оффтопа  
**Этап 2 (P0):** Technology.category + intents `databases_overview/languages_overview`  
**Этап 3 (P0):** `experience_by_company` + исправление кейса ALOR  
**Этап 4 (P0):** Verifier (grounding check) и rewrite/отказ  
**Этап 5 (P1):** Нормализация structured output + стабилизация лимитов/рендера  
**Этап 6 (P1):** Регресс‑набор тестов и автопрогон в CI

---

## 7) Что НЕ делаем в этом цикле (чтобы не расползлось)
- Полноценный Graph‑RAG с “reasoning over edges” и сложными алгоритмами.
- Обучение/дообучение модели.
- UI/дизайн фронта.

---


