services:
  chroma:
    image: ghcr.io/chroma-core/chroma:latest
    restart: unless-stopped
    environment:
      CHROMA_SERVER_AUTH_ENABLED: "false"
    ports: ["${CHROMA_PORT:-8001}:8000"]
    command: ["run", "--host", "0.0.0.0", "--port", "8000"]
    volumes:
      - chroma_data:/data
    healthcheck:
      test: [ "CMD", "curl", "-fsS", "http://localhost:8000/api/v1/heartbeat" ]
      interval: 5s
      timeout: 3s
      retries: 30
    logging:
      driver: json-file
      options: { max-size: "10m", max-file: "3" }

  # ЛОКАЛЬНЫЙ vLLM (опционально: если есть CUDA/GPU)
  vllm:
    image: vllm/vllm-openai:latest
    restart: unless-stopped
    environment:
      NVIDIA_VISIBLE_DEVICES: all
      VLLM_WORKER_USE_STDOUT: "1"
      HUGGING_FACE_HUB_TOKEN: ${HF_TOKEN}
    ports: [ "${VLLM_PORT:-8002}:8000" ]
    runtime: nvidia        # <- используйте вместо gpus:all, если нужно в WSL/старом докере
    command: >
      --model Qwen/Qwen2.5-7B-Instruct-AWQ
      --host 0.0.0.0 --port 8000
      --dtype auto
      --quantization awq_marlin
      --max-model-len 16384
      --gpu-memory-utilization 0.90
      --download-dir /root/.cache/hf
      --max-num-batched-tokens 8192
      --served-model-name qwen2.5-7b-instruct
      --trust-remote-code
      --enable-auto-tool-choice
      --tool-call-parser hermes
    volumes:
      - hf_cache:/root/.cache/hf

  # Embeddings сервер (OpenAI-совместимый endpoint /v1/embeddings)
  tei:
    image: ghcr.io/huggingface/text-embeddings-inference:cpu-1.5
    restart: unless-stopped
    environment:
      MODEL_ID: /data/model
      HF_ENDPOINT: ""
      HUGGINGFACE_HUB_BASE_URL: ""
      HF_HUB_ENABLE_HF_TRANSFER: "0"
    ports:
      - "${TEI_PORT:-8006}:80"
    volumes:
      - ./models/intfloat/multilingual-e5-base:/data/model:ro

  # ЕДИНЫЙ ПРОКСИ — LiteLLM
  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    restart: unless-stopped
    ports: ["${LITELLM_PORT:-8005}:4000"]
    environment:
      LITELLM_MASTER_KEY: ${LITELLM_MASTER_KEY}
      # сюда при необходимости добавишь ключи внешних провайдеров (OpenAI, GigaChat и т.п.)
    volumes:
      - ./litellm/config.yaml:/app/config.yaml:ro
    command: ["--config", "/app/config.yaml", "--port", "4000"]
    depends_on:
      - vllm
      - tei

  # content-api:
  #   build:
  #     context: ../services/content-api
  #     dockerfile: Dockerfile
  #   restart: unless-stopped
  #   environment:
  #     # подключаемся к postgres из другого compose-стека
  #     DATABASE_URL: postgresql+psycopg://ai_portfolio_user:ai-portfolio123@host.docker.internal:5433/ai_portfolio
  #     FRONTEND_ORIGIN: ${FRONTEND_ORIGIN:-http://localhost:3000}
  #     LOG_LEVEL: INFO
  #   command: >
  #     sh -lc "alembic upgrade head &&
  #             uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload"
  #   ports: [ "${CONTENT_PORT:-8003}:8000" ]
  #   healthcheck:
  #     test: [ "CMD", "curl", "-fsS", "http://localhost:8000/healthz" ]
  #     interval: 5s
  #     timeout: 3s
  #     retries: 30

  content-api:
    build:
      context: ../services/content-api-new
      dockerfile: Dockerfile
    restart: unless-stopped
    environment:
      DATABASE_URL: postgresql+psycopg://${POSTGRES_USER}:${POSTGRES_PASSWORD}@host.docker.internal:${POSTGRES_PORT}/${POSTGRES_DB}
      FRONTEND_ORIGIN: ${FRONTEND_ORIGIN:-http://localhost:3000}
      LOG_LEVEL: INFO
      APP_ENV: dev
    command: >
      sh -lc "alembic upgrade head &&
              uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload"
    ports: [ "${CONTENT_PORT:-8003}:8000" ]
    healthcheck:
      test: [ "CMD", "curl", "-fsS", "http://localhost:8000/healthz" ]
      interval: 5s
      timeout: 3s
      retries: 30

  # RAG backend — смотрит на LiteLLM
  rag-api:
    build:
      context: ../services/rag-api
      dockerfile: Dockerfile
    restart: unless-stopped
    environment:
      # === важно: тут переменные под твой текущий Settings ===
      litellm_base_url: http://litellm:4000/v1
      litellm_api_key: ${LITELLM_MASTER_KEY}
      chat_model: ${CHAT_MODEL}
      embedding_model: ${EMBEDDING_MODEL}
      giga_auth_data: ${GIGA_AUTH_DATA}

      # Chroma внутри сети Docker: порт ВСЕГДА 8000
      CHROMA_HOST: chroma
      CHROMA_PORT: 8000

      FRONTEND_ORIGIN: ${FRONTEND_ORIGIN}
      FRONTEND_LOCAL_IP: ${FRONTEND_LOCAL_IP}
      LOG_LEVEL: INFO
    depends_on:
      - chroma
      - litellm
    ports: ["${RAG_PORT:-8004}:8000"]

volumes:
  tei_cache:
  chroma_data:
  hf_cache:
