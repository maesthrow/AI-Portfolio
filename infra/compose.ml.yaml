version: "3.8"
services:
  vllm:
    image: vllm/vllm-openai:latest
    restart: unless-stopped
    environment:
      VLLM_WORKER_USE_STDOUT: "1"
    command: >
      --model meta-llama/Meta-Llama-3-8B-Instruct
      --max-num-batched-tokens 8192
      --download-dir /root/.cache/hf
      --host 0.0.0.0 --port 8000
    ports: ["8002:8000"]
    volumes:
      - hf_cache:/root/.cache/hf
    # ⚠️ без ключей gpus/device_requests, иначе снова валидатор спотыкается.
    # Когда будет нужен GPU:
    # - Linux: `docker run --gpus all ...` (через отдельный запуск),
    # - или держать vLLM вне compose (отдельным скриптом).
    logging:
      driver: json-file
      options: { max-size: "10m", max-file: "3" }

volumes:
  hf_cache:
